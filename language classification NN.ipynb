{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "#Read in full dataset\n",
    "data = pd.read_csv('sentences.csv',\n",
    "                            sep='\\t',\n",
    "                            encoding='utf8',\n",
    "                            index_col=0,\n",
    "                            names=['lang','text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m-m20\\AppData\\Local\\Temp\\ipykernel_496\\3822900035.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\m-m20\\AppData\\Local\\Temp\\ipykernel_496\\3822900035.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\m-m20\\AppData\\Local\\Temp\\ipykernel_496\\3822900035.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\m-m20\\AppData\\Local\\Temp\\ipykernel_496\\3822900035.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\m-m20\\AppData\\Local\\Temp\\ipykernel_496\\3822900035.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\m-m20\\AppData\\Local\\Temp\\ipykernel_496\\3822900035.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n"
     ]
    }
   ],
   "source": [
    "#Filter by text length\n",
    "len_cond = [True if 20<=len(s)<=200 else False for s in data['text']]\n",
    "data = data[len_cond]\n",
    "\n",
    "#Filter by text language\n",
    "lang = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
    "data = data[data['lang'].isin(lang)]\n",
    "\n",
    "#Select 50000 rows for each language\n",
    "data_trim = pd.DataFrame(columns=['lang','text'])\n",
    "\n",
    "for l in lang:\n",
    "    lang_trim = data[data['lang'] ==l].sample(50000,random_state = 100)\n",
    "    data_trim = data_trim.append(lang_trim)\n",
    "\n",
    "#Create a random train, valid, test split\n",
    "data_shuffle = data_trim.sample(frac=1)\n",
    "\n",
    "train = data_shuffle[0:210000]\n",
    "valid = data_shuffle[210000:270000]\n",
    "test = data_shuffle[270000:300000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def get_trigrams(corpus,n_feat=200):\n",
    "    \"\"\"\n",
    "    Returns a list of the N most common character trigrams from a list of sentences\n",
    "    params\n",
    "    ------------\n",
    "        corpus: list of strings\n",
    "        n_feat: integer\n",
    "    \"\"\"\n",
    "\n",
    "    #fit the n-gram model\n",
    "    vectorizer = CountVectorizer(analyzer='char',\n",
    "                            ngram_range=(3, 3)\n",
    "                            ,max_features=n_feat)\n",
    "\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    #Get model feature names\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    return feature_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#obtain trigrams from each language\n",
    "features = {}\n",
    "features_set = set()\n",
    "\n",
    "for l in lang:\n",
    "\n",
    "    #get corpus filtered by language\n",
    "    corpus = train[train.lang==l]['text']\n",
    "\n",
    "    #get 200 most frequent trigrams\n",
    "    trigrams = get_trigrams(corpus)\n",
    "\n",
    "    #add to dict and set\n",
    "    features[l] = trigrams\n",
    "    features_set.update(trigrams)\n",
    "\n",
    "\n",
    "#create vocabulary list using feature set\n",
    "vocab = dict()\n",
    "for i,f in enumerate(features_set):\n",
    "    vocab[f]=i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "#train count vectoriser using vocabulary\n",
    "vectorizer = CountVectorizer(analyzer='char',\n",
    "                             ngram_range=(3, 3),\n",
    "                            vocabulary=vocab)\n",
    "\n",
    "#create feature matrix for training set\n",
    "corpus = train['text']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "train_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "#Scale feature matrix\n",
    "train_min = train_feat.min()\n",
    "train_max = train_feat.max()\n",
    "train_feat = (train_feat - train_min)/(train_max-train_min)\n",
    "\n",
    "#Add target variable\n",
    "train_feat['lang'] = list(train['lang'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "#create feature matrix for validation set\n",
    "corpus = valid['text']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "valid_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
    "valid_feat = (valid_feat - train_min)/(train_max-train_min)\n",
    "valid_feat['lang'] = list(valid['lang'])\n",
    "\n",
    "#create feature matrix for test set\n",
    "corpus = test['text']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "test_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
    "test_feat = (test_feat - train_min)/(train_max-train_min)\n",
    "test_feat['lang'] = list(test['lang'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "#Fit encoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(['deu', 'eng', 'fra', 'ita', 'por', 'spa'])\n",
    "\n",
    "def encode(y):\n",
    "    \"\"\"\n",
    "    Returns a list of one hot encodings\n",
    "    Params\n",
    "    ---------\n",
    "        y: list of language labels\n",
    "    \"\"\"\n",
    "\n",
    "    y_encoded = encoder.transform(y)\n",
    "    y_dummy = np_utils.to_categorical(y_encoded)\n",
    "\n",
    "    return y_dummy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 667), found shape=(100, 671)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_496\\976680685.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;31m#Train model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     68\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mautograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m           \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint:disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1146\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ag_error_metadata\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1147\u001B[1;33m               \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1148\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1149\u001B[0m               \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\m-m20\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 667), found shape=(100, 671)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Get training data\n",
    "x = train_feat.drop('lang',axis=1)\n",
    "y = encode(train_feat['lang'])\n",
    "\n",
    "#Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=667, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Train model\n",
    "model.fit(x, y, epochs=4, batch_size=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_test = test_feat.drop('lang',axis=1)\n",
    "y_test = test_feat['lang']\n",
    "\n",
    "#Get predictions on test set\n",
    "labels = np.argmax(model.predict(x_test), axis=-1)\n",
    "predictions = encoder.inverse_transform(labels)\n",
    "\n",
    "#Accuracy on test set\n",
    "accuracy = accuracy_score(y_test,predictions)\n",
    "print(accuracy)\n",
    "\n",
    "#Create confusion matrix\n",
    "lang = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
    "conf_matrix = confusion_matrix(y_test,predictions)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix,columns=lang,index=lang)\n",
    "\n",
    "#Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(conf_matrix_df,cmap='coolwarm',annot=True,fmt='.5g',cbar=False)\n",
    "plt.xlabel('Predicted',fontsize=22)\n",
    "plt.ylabel('Actual',fontsize=22)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}